{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37744891",
   "metadata": {},
   "source": [
    "# Image Denoising Challenge\n",
    "\n",
    "The goal for this challenge is to leverage your knowledge of Deep Learning to design and train a denoising model. For a given noisy image $X$, our model should learn to predict the denoised image $y$.\n",
    "\n",
    "\n",
    "**Objectives**\n",
    "- Visualize images\n",
    "- Preprocess images for the neural network\n",
    "- Fit a custom CNN for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc3314",
   "metadata": {},
   "source": [
    "## 1. Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b59a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# open the image\n",
    "from PIL import Image\n",
    "from scipy.fftpack import dct\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d0b0e",
   "metadata": {},
   "source": [
    "## 2. Load Data\n",
    "\n",
    "üëâ Let's download the dataset archive.\n",
    "It contains RGB and Black & White images we will be using for the rest of this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e468dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf paintings/\n",
    "! curl https://wagon-public-datasets.s3.amazonaws.com/certification_france_2021_q2/paintings.zip > paintings.zip\n",
    "! LC_ALL=UTF-8 unzip -q \"paintings.zip\"\n",
    "! rm \"paintings.zip\"\n",
    "! ls -l paintings/Albrecht*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c1de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !(cd paintings && find . -type f -exec sh -c 'mv \"$1\" \"$(echo \"$1\" | iconv -f cp862 -t utf8)\"' sh {} \\;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab995a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_paths = glob.glob(\"./paintings/*.jpg\")\n",
    "print(f\"The dataset contains {len(dataset_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd613d",
   "metadata": {},
   "source": [
    "‚ùì **Display the image at index `53` of this dataset_paths (i.e the 54-th image)**\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "    Use the <code>PIL.Image.open</code> and <code>matplotlib.pyplot.imshow</code> functions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccd91f4",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# get the path of the image at index 53\n",
    "path_53 = dataset_paths[53]\n",
    "\n",
    "with Image.open(path_53) as im:\n",
    "    plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1876cbb6",
   "metadata": {},
   "source": [
    "‚ùì **What is the shape of the image you displayed above `img_shape`?  How many dimensions `img_dim` does it have ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8eb466",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# shape of the image\n",
    "image = Image.open(path_53)\n",
    "# size is given (width, height)\n",
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65034f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed so that results are reproduceable\n",
    "np.random.seed(42)\n",
    "\n",
    "np_image = np.array(im)\n",
    "# (row, col, depth)\n",
    "np_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936dccee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as it is a colourful image -> 3 dimensions for RGB\n",
    "img_dim = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb31d38f",
   "metadata": {},
   "source": [
    "‚ùì **What was in the image above?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = np_image.shape\n",
    "img_dim = img_dim\n",
    "\n",
    "# is_portrait = True\n",
    "is_portrait = False\n",
    "\n",
    "is_colored_image = True\n",
    "# is_colored_image = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae2cdc",
   "metadata": {},
   "source": [
    "## 3. Look for duplicates\n",
    "\n",
    "üëâ Let's check if we have similar images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hasher:\n",
    "    def __init__(self, verbose: bool = False) -> None:\n",
    "        self.target_size = (32, 32)\n",
    "        self.__coefficient_extract = (8, 8)\n",
    "        self.verbose = verbose\n",
    "        self.query_results_map = None\n",
    "\n",
    "    @staticmethod\n",
    "    def hamming_distance(hash1: str, hash2: str) -> float:\n",
    "        hash1_bin = bin(int(hash1, 16))[2:].zfill(64)\n",
    "        hash2_bin = bin(int(hash2, 16))[2:].zfill(64)\n",
    "        return np.sum([i != j for i, j in zip(hash1_bin, hash2_bin)])\n",
    "\n",
    "    def encode_image(self, image_file=None) -> str:\n",
    "        try:\n",
    "            if image_file and os.path.exists(image_file):\n",
    "                image_file = Path(image_file)\n",
    "                img = Image.open(image_file)\n",
    "\n",
    "                if img.format not in [\n",
    "                    \"JPEG\",\n",
    "                    \"PNG\",\n",
    "                    \"BMP\",\n",
    "                    \"MPO\",\n",
    "                    \"PPM\",\n",
    "                    \"TIFF\",\n",
    "                    \"GIF\",\n",
    "                ]:\n",
    "                    return None\n",
    "\n",
    "                if img.mode != \"RGB\":\n",
    "                    # convert to RGBA first to avoid warning\n",
    "                    # we ignore alpha channel if available\n",
    "                    img = img.convert(\"RGBA\").convert(\"RGB\")\n",
    "\n",
    "                image_pp = np.array(\n",
    "                    img.resize(self.target_size, Image.Resampling.LANCZOS).convert(\"L\")\n",
    "                ).astype(\"uint8\")\n",
    "            else:\n",
    "                raise ValueError\n",
    "        except (ValueError, TypeError):\n",
    "            raise ValueError(\"Please provide either image file path\")\n",
    "\n",
    "        return self._hash_func(image_pp) if isinstance(image_pp, np.ndarray) else None\n",
    "\n",
    "    def encode_images(self, image_dir=None) -> Dict[str, str]:\n",
    "        if not os.path.isdir(image_dir):\n",
    "            raise ValueError(\"Please provide a valid directory path!\")\n",
    "\n",
    "        image_dir = Path(image_dir)\n",
    "\n",
    "        files = [\n",
    "            i.absolute() for i in image_dir.glob(\"*\") if not i.name.startswith(\".\")\n",
    "        ]  # ignore hidden files\n",
    "\n",
    "        # pool = Pool(processes=cpu_count())\n",
    "        # hashes = list(\n",
    "        #    tqdm.tqdm(pool.imap(self.encode_image, files, 100), total=len(files), disable=not self.verbose)\n",
    "        # )\n",
    "        # pool.close()\n",
    "        # pool.join()\n",
    "\n",
    "        hashes = [self.encode_image(f) for f in files]\n",
    "\n",
    "        hash_initial_dict = dict(zip([f.name for f in files], hashes))\n",
    "        hash_dict = {\n",
    "            k: v for k, v in hash_initial_dict.items() if v\n",
    "        }  # To ignore None (returned if some probelm with image file)\n",
    "\n",
    "        return hash_dict\n",
    "\n",
    "    def _hash_func(self, image_array: np.ndarray) -> str:\n",
    "        dct_coef = dct(dct(image_array, axis=0), axis=1)\n",
    "\n",
    "        # retain top left 8 by 8 dct coefficients\n",
    "        dct_reduced_coef = dct_coef[\n",
    "            : self.__coefficient_extract[0], : self.__coefficient_extract[1]\n",
    "        ]\n",
    "\n",
    "        # median of coefficients excluding the DC term (0th term)\n",
    "        median_coef_val = np.median(np.ndarray.flatten(dct_reduced_coef)[1:])\n",
    "\n",
    "        # return mask of all coefficients greater than mean of coefficients\n",
    "        hash_mat = dct_reduced_coef >= median_coef_val\n",
    "\n",
    "        return \"\".join(\"%0.2x\" % x for x in np.packbits(hash_mat))\n",
    "\n",
    "    def find_duplicates(\n",
    "        self, image_dir: str = None, encoding_map: List[str] = None, threshold: int = 10\n",
    "    ) -> Dict:\n",
    "        if not encoding_map:\n",
    "            encoding_map = self.encode_images(image_dir)\n",
    "\n",
    "        result_map = defaultdict(list)\n",
    "\n",
    "        for i, (ki, vi) in enumerate(encoding_map.items()):\n",
    "            for j, (kj, vj) in enumerate(encoding_map.items()):\n",
    "                if i < j:\n",
    "                    dij = self.hamming_distance(vi, vj)\n",
    "                    if dij <= threshold:\n",
    "                        result_map[ki].append((kj, dij))\n",
    "                        result_map[kj].append((ki, dij))\n",
    "\n",
    "        self.query_results_map = {\n",
    "            k: [i for i in sorted(v, key=lambda tup: tup[1], reverse=False)]\n",
    "            for k, v in result_map.items()\n",
    "        }\n",
    "\n",
    "        return self.query_results_map\n",
    "\n",
    "    def get_files_to_remove(self, duplicates: Dict[str, List] = None) -> List:\n",
    "        files_to_remove = set()\n",
    "\n",
    "        for k, v in duplicates.items():\n",
    "            tmp = [i[0].encode(encoding=\"UTF-8\", errors=\"strict\") for i in v]\n",
    "\n",
    "            if k not in files_to_remove:\n",
    "                files_to_remove.update(tmp)\n",
    "\n",
    "        return list(files_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hasher = Hasher()\n",
    "encodings = hasher.encode_images(\"paintings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc3f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = hasher.find_duplicates(encoding_map=encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97423a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = \"paintings\"\n",
    "\n",
    "for original, image_list in duplicates.items():\n",
    "    n_images = len(image_list)\n",
    "    ncols = 4  # fixed for a consistent layout\n",
    "    nrows = int(np.ceil(n_images / ncols)) + 1\n",
    "    fig = plt.Figure(figsize=(10, 14))\n",
    "\n",
    "    gs = gridspec.GridSpec(nrows=nrows, ncols=ncols)\n",
    "    ax = plt.subplot(\n",
    "        gs[0, 1:3]\n",
    "    )  # Always plot the original image in the middle of top row\n",
    "    ax.imshow(Image.open(f\"{image_dir}/{original}\"))\n",
    "    ax.set_title(f\"Original Image: {original}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    for i in range(0, n_images):\n",
    "        row_num = (i // ncols) + 1\n",
    "        col_num = i % ncols\n",
    "\n",
    "        ax = plt.subplot(gs[row_num, col_num])\n",
    "        ax.imshow(Image.open(f\"{image_dir}/{image_list[i][0]}\"))\n",
    "        title = \" \".join([image_list[i][0], f\"({image_list[i][1]})\"])\n",
    "        ax.set_title(title, fontsize=9)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011da0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hasher.get_files_to_remove(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e288509",
   "metadata": {},
   "source": [
    "## 4. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a140c68",
   "metadata": {},
   "source": [
    "‚ùì **Store all images from the dataset folder in a list of numpy arrays called `dataset_images`**\n",
    "\n",
    "- It can take a while\n",
    "- If the dataset is too big to fit in memory, just take the first half (or quarter) of all pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec89764",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "dataset_images = []\n",
    "\n",
    "for img_name in dataset_paths:\n",
    "    image = Image.open(img_name)\n",
    "    dataset_images.append(np.array(image))\n",
    "\n",
    "print(type(dataset_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db741ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_images), dataset_images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6e7e62",
   "metadata": {},
   "source": [
    "### 4.1 Reshape, Resize, Rescale\n",
    "\n",
    "Let's simplify our dataset and convert it to a single numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a315e1a",
   "metadata": {},
   "source": [
    "‚ùì **First, check if that all the images in the dataset have the same number of dimensions**.\n",
    "- What do you notice?\n",
    "- How do you explain it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e700a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1, dim2, dim3 = [], [], []\n",
    "for img in dataset_images:\n",
    "    shap = img.shape\n",
    "    dim1.append(shap[0])\n",
    "    dim2.append(shap[1])\n",
    "    if len(shap) > 2:\n",
    "        dim3.append(shap[2])\n",
    "\n",
    "if len(dim3) == len(dim2):\n",
    "    print(\"All images are RGB\")\n",
    "else:\n",
    "    print(\"Some images are B&W\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b60b6d",
   "metadata": {},
   "source": [
    "üëâ We convert for you all black & white images into 3-colored ones by duplicating the image on three channels, so as to have only 3D arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ef5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_images = [\n",
    "    x if x.ndim == 3 else np.repeat(x[:, :, None], 3, axis=2)\n",
    "    for x in tqdm(dataset_images)\n",
    "]\n",
    "\n",
    "# Print the set of the number of dimensions of all arrays (it should contain only the value 3)\n",
    "set([x.ndim for x in dataset_images])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c34d7d7",
   "metadata": {},
   "source": [
    "‚ùì **What about their shape now ?**\n",
    "- Do they all have the same width/heights ? If not:\n",
    "- Resize the images (120 pixels height and 100 pixels width) in the dataset, using `tensorflow.image.resize` function.\n",
    "- Now that they all have the same shape, store them as a numpy array `dataset_resized`.\n",
    "- This array should thus be of size $(n_{images}, 120, 100, 3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7edd8a",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "if len(set(dim1)) == 1 and len(set(dim2)) == 1:\n",
    "    print(\"Images dont have all the same size\")\n",
    "else:\n",
    "    print(\"Images dont have the same size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe158aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[height]\", \"max:\", max(dim1), \"min:\", min(dim1))\n",
    "print(\"[width] \", \"max:\", max(dim2), \"min:\", min(dim2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cf09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4ff6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# resizing all\n",
    "\n",
    "resized_img = []\n",
    "\n",
    "for img in dataset_images:\n",
    "    resized_img.append(resize(img, (120, 100), antialias=True).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837efb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_resized = np.array(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ec092",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_resized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad5ff7",
   "metadata": {},
   "source": [
    "‚ùì **Rescale the data of each image between $0$ and $1$**\n",
    "- Save your resulting list as `dataset_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255087e7",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "print(np.max(dataset_resized), np.min(dataset_resized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfd462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide by 255\n",
    "dataset_scaled = dataset_resized / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367bc23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the max and mean:\n",
    "print(np.min(dataset_scaled), np.max(dataset_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bcba81",
   "metadata": {},
   "source": [
    "### 4.2 Create (X, y) sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433bfb7e",
   "metadata": {},
   "source": [
    "üëâ Now, we'll add for you some **random noise** to our images to simulate noise (that our model will try to remove later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce71c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOISE_LEVEL = 0.2\n",
    "\n",
    "# Compute Gaussian noise\n",
    "mean = 0\n",
    "var = 0.1\n",
    "sigma = var**0.5\n",
    "noise = NOISE_LEVEL * np.random.normal(mean, sigma, size=dataset_scaled.shape)\n",
    "\n",
    "# There are other types of noise: poisson, speckle, salt_n_pepper\n",
    "\n",
    "print(type(noise), noise.shape, noise.dtype)\n",
    "dataset_noisy = dataset_scaled + noise\n",
    "\n",
    "# Images data can now be in [-0.2, 1.2]\n",
    "print(f\"Before clipping: [{np.min(dataset_noisy)}, {np.max(dataset_noisy)}]\")\n",
    "\n",
    "dataset_noisy = np.clip(dataset_noisy, 0, 1)\n",
    "print(f\"After clipping: [{np.min(dataset_noisy), np.max(dataset_noisy)}]\")\n",
    "\n",
    "dataset_noisy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbd008",
   "metadata": {},
   "source": [
    "‚ùì **Plot a noisy image below to visualize the noise and compare it with the normal one**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629856c",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "for i, (scaled_image, noisy_image) in enumerate(zip(dataset_scaled, dataset_noisy)):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10))\n",
    "    ax1.imshow(scaled_image)\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(noisy_image)\n",
    "    ax2.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dfcfb0",
   "metadata": {},
   "source": [
    "‚ùì **Create your `(X_train, Y_train)`, `(X_test, Y_test)` training set for your problem**\n",
    "\n",
    "- Remember you are trying to use \"noisy\" pictures in order to predict the \"normal\" ones.\n",
    "- Keeping about `20%` of randomly sampled data as test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6c7a9e",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X = dataset_noisy\n",
    "y = dataset_scaled\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b846231",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    X_train.shape, Y_train.shape, X_train[:, :, :, 0].std(), Y_train[:, :, :, 0].std()\n",
    ")\n",
    "# Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd012291",
   "metadata": {},
   "source": [
    "## 5. Convolutional Neural Network\n",
    "\n",
    "A commonly used neural network architecture for image denoising is the __AutoEncoder__.\n",
    "\n",
    "<img src='https://github.com/lewagon/data-images/blob/master/DL/autoencoder.png?raw=true'>\n",
    "\n",
    "Its goal is to learn a compact representation of your data to reconstruct them as precisely as possible.  \n",
    "The loss for such model must incentivize it to have __an output as close to the input as possible__.\n",
    "\n",
    "For this challenge, __you will only be asked to code the Encoder part of the network__, since building a Decoder leverages layers architectures you are not familiar with (yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461f7ee6",
   "metadata": {},
   "source": [
    "üëâ Run this code below if you haven't managed to build your own (X,Y) training sets. This will load them as solution\n",
    "\n",
    "```python\n",
    "! curl https://wagon-public-datasets.s3.amazonaws.com/certification_france_2021_q2/data_painting_solution.pickle > data_painting_solution.pickle\n",
    "\n",
    "import pickle\n",
    "with open(\"data_painting_solution.pickle\", \"rb\") as file:\n",
    "    (X_train, Y_train, X_test, Y_test) = pickle.load(file)\n",
    "    \n",
    "! rm data_painting_solution.pickle\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5ec696",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl https://wagon-public-datasets.s3.amazonaws.com/certification_france_2021_q2/data_painting_solution.pickle > data_painting_solution.pickle\n",
    "\n",
    "with open(\"data_painting_solution.pickle\", \"rb\") as file:\n",
    "    (X_train_ref, Y_train_ref, X_test_ref, Y_test_ref) = pickle.load(file)\n",
    "\n",
    "! rm data_painting_solution.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b63fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_display = [[50, 50], [62, 110], [228, 207], [187, 322], [238, 8]]\n",
    "\n",
    "for i, pair in enumerate(to_display):\n",
    "    _, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(20, 20))\n",
    "    ax1.imshow(Y_train[pair[0]])\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(Y_train_ref[pair[1]])\n",
    "    ax2.axis(\"off\")\n",
    "    ax3.imshow(Y_train[pair[0]] - Y_train_ref[pair[1]])\n",
    "    ax3.axis(\"off\")\n",
    "    ax4.imshow(X_train[pair[0]])\n",
    "    ax4.axis(\"off\")\n",
    "    ax5.imshow(X_train_ref[pair[1]])\n",
    "    ax5.axis(\"off\")\n",
    "    print(\n",
    "        f\"{np.sum(Y_train[pair[0]]-Y_train_ref[pair[1]])} - {X_train[pair[0],:, :, 0].std()} vs {X_train_ref[pair[1],:, :, 0].std()}\"\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c66399",
   "metadata": {},
   "source": [
    "### 5.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789e244a",
   "metadata": {},
   "source": [
    "üëâ Run the cell below that defines the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose to compress images into a latent_dimension of size 6000\n",
    "latent_dimensions = 6000\n",
    "\n",
    "# We build a decoder that takes 1D-vectors of size 6000 to reconstruct images of shape (120,100,3)\n",
    "decoder = Sequential(name=\"decoder\")\n",
    "decoder.add(layers.Reshape((30, 25, 8), input_dim=latent_dimensions))\n",
    "decoder.add(\n",
    "    layers.Conv2DTranspose(\n",
    "        filters=16, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "decoder.add(\n",
    "    layers.Conv2DTranspose(\n",
    "        filters=32, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"\n",
    "    )\n",
    ")\n",
    "decoder.add(\n",
    "    layers.Conv2D(filters=3, kernel_size=3, padding=\"same\", activation=\"sigmoid\")\n",
    ")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f497ffa",
   "metadata": {},
   "source": [
    "‚ùì **Now, build the `encoder` that plugs correctly with the decoder defined above**. Make sure that:\n",
    "- The output of your `encoder` is the same shape as the input of the `decoder`\n",
    "- Use a convolutional neural network architecture without transfer learning\n",
    "- Keep it simple\n",
    "- Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_encoder(latent_dimension):\n",
    "    encoder = Sequential(name=\"encoder\")\n",
    "    encoder.add(layers.InputLayer(input_shape=(120, 100, 3)))\n",
    "    encoder.add(\n",
    "        layers.Conv2D(\n",
    "            32, 5, strides=2, padding=\"same\", activation=layers.LeakyReLU(0.1)\n",
    "        )\n",
    "    )\n",
    "    encoder.add(layers.Dropout(0.2))\n",
    "    encoder.add(\n",
    "        layers.Conv2D(8, 3, strides=2, padding=\"same\", activation=layers.LeakyReLU(0.1))\n",
    "    )\n",
    "    encoder.add(layers.Dropout(0.3))\n",
    "    encoder.add(layers.Flatten())\n",
    "    # encoder.add(Dense(latent_dimension, activation='sigmoid'))\n",
    "\n",
    "    return encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = build_encoder(6000)\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ec2cc0",
   "metadata": {},
   "source": [
    "üëâ **Test your encoder below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd12432",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Input(shape=(120, 100, 3))\n",
    "autoencoder = Model(x, decoder(encoder(x)), name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4e034",
   "metadata": {},
   "source": [
    "### 5.2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34034ac",
   "metadata": {},
   "source": [
    "‚ùì **Before training the autoencoder, evaluate your baseline score**\n",
    "- We will use the mean absolute error in this challenge\n",
    "- Compute the baseline score on your test set in the \"stupid\" case where you don't manage to de-noise anything at all.\n",
    "- Store the result under `score_baseline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128449e",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "class Baseline(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Baseline, self).__init__()\n",
    "\n",
    "    def call(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "baseline = Baseline()\n",
    "\n",
    "baseline.compile(loss=\"binary_crossentropy\", optimizer=\"adam\")\n",
    "\n",
    "# An other way to do it !\n",
    "# print(tf.keras.metrics.mean_absolute_error(Y_test, X_test).numpy().mean())\n",
    "\n",
    "score_baseline = baseline.evaluate(X_test, Y_test)\n",
    "score_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecb73d",
   "metadata": {},
   "source": [
    "‚ùì Now, **train your autoencoder**\n",
    "\n",
    "- Use an appropriate loss\n",
    "- Adapt the learning rate of your optimizer if convergence is too slow/fast\n",
    "- Make sure your model does not overfit with appropriate control techniques\n",
    "\n",
    "üí° You will not be judged by the computing power of your computer, you can reach decent performance in less than 5 minutes of training without GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b352e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_autoencoder(autoencoder):\n",
    "    autoencoder.compile(loss=\"mae\", optimizer=tf.optimizers.Adam(0.0005))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff44c49",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=25, verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "compile_autoencoder(autoencoder)\n",
    "\n",
    "history_denoising = autoencoder.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=1000,\n",
    "    shuffle=True,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15566948",
   "metadata": {},
   "source": [
    "‚ùì **Plot your training and validation loss at each epoch using the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12101fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, title=\"\", axs=None, exp_name=\"\"):\n",
    "    f = plt.figure(figsize=(10, 7))\n",
    "    f.add_subplot()\n",
    "\n",
    "    # Adding Subplot\n",
    "    plt.plot(\n",
    "        history.epoch, history.history[\"loss\"], label=\"loss\"\n",
    "    )  # Loss curve for training set\n",
    "    plt.plot(\n",
    "        history.epoch, history.history[\"val_loss\"], label=\"val_loss\"\n",
    "    )  # Loss curve for validation set\n",
    "\n",
    "    plt.title(\"Loss Curve\", fontsize=18)\n",
    "    plt.xlabel(\"Epochs\", fontsize=15)\n",
    "    plt.ylabel(\"Loss\", fontsize=15)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    # plt.savefig(\"Loss_curve.png\")\n",
    "    return f\n",
    "\n",
    "\n",
    "plot_history(history_denoising)\n",
    "\n",
    "\n",
    "# Run also this code to save figure as jpg in path below (it's your job to ensure it works)\n",
    "fig = plt.gcf()\n",
    "plt.savefig(\"history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9101b81d",
   "metadata": {},
   "source": [
    "‚ùì **Evaluate your performances on test set**\n",
    "- Compute your de-noised test set `Y_pred` \n",
    "- Store your test score as `score_test`\n",
    "- Plot a de-noised image from your test set and compare it with the original and noisy one using the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1003f92",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "score_test = autoencoder.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4905b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = autoencoder.predict(X_test)\n",
    "\n",
    "l = np.arange(len(X_test))\n",
    "np.random.shuffle(l)\n",
    "\n",
    "for i in l[:5]:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\n",
    "    ax1.imshow(Y_test[i])\n",
    "    ax1.set_title(\"Clean image.\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(X_test[i])\n",
    "    ax2.set_title(\"Noisy image.\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    ax3.imshow(Y_pred[i])\n",
    "    ax3.set_title(\"Prediction.\")\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "# Run this to save your results for correction\n",
    "plt.savefig(\"image_denoised.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15384628",
   "metadata": {},
   "source": [
    "## 6. Other stuff (Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c778a5",
   "metadata": {},
   "source": [
    "### 6.1 Use dataset from numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = tf.data.Dataset.from_tensor_slices(())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece29994",
   "metadata": {},
   "source": [
    "### 6.2 Use dataset from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a66b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = glob.glob(\"paintings/*.jpg\")\n",
    "\n",
    "n_full = len(list_files)\n",
    "\n",
    "n_train = int(0.7 * n_full)\n",
    "n_test = int(0.15 * n_full)\n",
    "n_val = n_full - n_train - n_test\n",
    "\n",
    "print(f\"n_train: {n_train} - n_val: {n_val} - n_test: {n_test}\")\n",
    "\n",
    "ds_full = tf.data.Dataset.from_tensor_slices((list_files, list_files)).shuffle(\n",
    "    n_full, reshuffle_each_iteration=False\n",
    ")\n",
    "ds_train = ds_full.take(n_train)\n",
    "ds_test = ds_full.skip(n_train)\n",
    "ds_val = ds_test.skip(n_val)\n",
    "ds_test = ds_test.take(n_test)\n",
    "\n",
    "\n",
    "def load(file_path, add_noise=False):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, size=(120, 100), antialias=True)\n",
    "    if add_noise:\n",
    "        img += 0.2 * tf.random.normal(\n",
    "            (120, 100, 3), stddev=0.316, dtype=tf.dtypes.float32\n",
    "        )\n",
    "    img = tf.clip_by_value(img, 0, 1)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path1, file_path2):\n",
    "    img = tf.stack([load(file_path1), load(file_path2, add_noise=True)])\n",
    "    choice = tf.random.uniform((), minval=0, maxval=1)\n",
    "    if choice < 0.5:\n",
    "        img = tf.image.random_hue(img, max_delta=0.5)\n",
    "    choice = tf.random.uniform((), minval=0, maxval=1)\n",
    "    if choice < 0.5:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "    choice = tf.random.uniform((), minval=0, maxval=1)\n",
    "    if choice < 0.5:\n",
    "        img = tf.image.flip_up_down(img)\n",
    "\n",
    "    return img[0], img[1]\n",
    "\n",
    "\n",
    "ds_train = (\n",
    "    ds_train.map(\n",
    "        lambda x, y: process_image(x, y),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "    .shuffle(n_train)\n",
    "    .batch(64)\n",
    "    .prefetch(2)\n",
    ")\n",
    "ds_val = (\n",
    "    ds_val.map(\n",
    "        lambda x, y: process_image(x, y),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n",
    "    )\n",
    "    .shuffle(n_val)\n",
    "    .batch(64)\n",
    ")\n",
    "ds_test = ds_test.map(\n",
    "    lambda x, y: process_image(x, y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ").batch(64)\n",
    "\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "for (x, y) in ds_train.take(5):\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    print(np.min(x[0, ...]), np.max(x[0, ...]), np.min(y[0, ...]), np.max(y[0, ...]))\n",
    "    ax1.imshow(x[0, ...])\n",
    "    ax1.axis(\"off\")\n",
    "    ax2.imshow(y[0, ...])\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ba9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 6000\n",
    "\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.InputLayer(input_shape=(120, 100, 3)),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    32, 5, strides=2, padding=\"same\", activation=layers.LeakyReLU(0.1)\n",
    "                ),\n",
    "                tf.keras.layers.Dropout(0.2),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    8, 3, strides=2, padding=\"same\", activation=layers.LeakyReLU(0.1)\n",
    "                ),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Flatten(),\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = tf.keras.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Reshape((30, 25, 8), input_dim=self.latent_dim),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=16,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"same\",\n",
    "                    activation=\"relu\",\n",
    "                ),\n",
    "                tf.keras.layers.Conv2DTranspose(\n",
    "                    filters=32,\n",
    "                    kernel_size=3,\n",
    "                    strides=2,\n",
    "                    padding=\"same\",\n",
    "                    activation=\"relu\",\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=3, kernel_size=3, padding=\"same\", activation=\"sigmoid\"\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78209d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=\"adam\", loss=tf.keras.losses.MeanAbsoluteError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=25, verbose=1, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# reduce learning rate when on a plateau\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    min_delta=0.0001,\n",
    "    factor=0.2,\n",
    ")\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    ds_train,\n",
    "    epochs=1_000,\n",
    "    shuffle=True,\n",
    "    validation_data=ds_val,\n",
    "    callbacks=[es, reduce_lr],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b2fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f615132",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.evaluate(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8aadb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for (x, y) in ds_test:\n",
    "    _, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "    ax1.imshow(y[0, ...])\n",
    "    ax1.set_title(\"Clean image.\")\n",
    "    ax1.axis(\"off\")\n",
    "\n",
    "    ax2.imshow(x[0, ...])\n",
    "    ax2.set_title(\"Noisy image.\")\n",
    "    ax2.axis(\"off\")\n",
    "\n",
    "    y_pred = autoencoder.predict(x)\n",
    "\n",
    "    ax3.imshow(y_pred[i])\n",
    "    ax3.set_title(\"Prediction.\")\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
