{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52985d3-6a15-4385-8992-9ee738d4a0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b562554-4923-4a68-bf13-22d6e5b5732d",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9718ec-5e12-40a1-8abc-32ff85d78ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://raw.githubusercontent.com/akshayjoshii/COVID19-Tweet-Sentiment-Analysis-and-EDA/master/finalSentimentdata2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7792f4d3-4ed3-475a-b31e-266745db26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"finalSentimentdata2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004e28d9-4401-4cbc-85db-06dd48211511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df54ae3d-5c35-4e80-bbcb-0b4ed87e926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=df[\"sentiment\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1503bf9a-a889-4ad1-b4d8-f8797fdfb769",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95e4d0-8640-4832-aa24-6f160865f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1ff32-b74c-4f44-bcce-eef13e6d8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Make text lowercase\n",
    "    text = text.lower()\n",
    "    # Remove text within square brackets\n",
    "    text = re.sub(\"\\[.*?\\]\", \"\", text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    # Remove text within <>\n",
    "    text = re.sub(\"<.*?>+\", \"\", text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    # Remove newline\n",
    "    text = re.sub(\"\\n\", \"\", text)\n",
    "    # Remove words containing numbers\n",
    "    text = re.sub(\"\\w*\\d\\w*\", \"\", text)\n",
    "    # Remove unicode emojis (todo) but this could mean something!\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f603f01-6a0e-46e6-8e6c-72d7c284dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply `clean_text` to the text element of the dataframe\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b3a38-9a38-4c80-ad24-2713dad1f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the max number of words in the tweets\n",
    "\n",
    "df[\"n_words\"] = df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "df[\"n_words\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5e04a-1680-4879-b6b1-23118f5351ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achtung we have empty tweets\n",
    "\n",
    "df[\"n_words\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694f02db-dcad-4230-8af9-12e522503b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df[\"n_words\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94d542-10d0-4d52-98c9-359eb6ce30b0",
   "metadata": {},
   "source": [
    "# Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c2401-ccb5-4f46-ac3c-83f389a391ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab2552-f7da-4053-8640-7036645bbef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0ba46c-c3c3-4451-bbfe-9200356653be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    df[\"text\"], df[\"sentiment\"], test_size=0.30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612966d1-8e59-45b7-b37d-600eaeebf8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train.values.tolist())\n",
    "X_test = np.array(X_test.values.tolist())\n",
    "y_train = np.array(y_train.values.tolist()).reshape(-1, 1)\n",
    "y_test = np.array(y_test.values.tolist()).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd0363-1992-48a5-bf2f-b9e4efa9e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.to_categorical(df.sentiment) needs integer inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d098f7-a5c8-4162-b659-e3b808d3dc7d",
   "metadata": {},
   "source": [
    "# Encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf34c7e-9b9d-40be-823a-97dd1c36da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a96b9-4d2c-4cee-8d48-514a3e2718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:5], y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f99f1-3173-476c-b16d-f4a09ad9636a",
   "metadata": {},
   "source": [
    "## LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75503d53-182c-4d64-8152-96f30450c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y_train_lb = lb.fit_transform(y_train)  # pay attention to the []\n",
    "\n",
    "print(lb.classes_)\n",
    "\n",
    "y_test_lb = lb.transform(list(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952eb383-9d6c-4057-b278-852202d03cb9",
   "metadata": {},
   "source": [
    "## OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72410b91-b6c2-4182-9579-10abdb276217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ohe = OneHotEncoder(sparse=False, drop='first') # Remove the first column\n",
    "ohe = OneHotEncoder(sparse=False)  # Is needed for TF\n",
    "y_train_ohe = ohe.fit_transform(y_train)  # pay attention to the []\n",
    "\n",
    "y_test_ohe = ohe.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600086e-e13b-44e5-93fc-fd07790df7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_ohe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e715ea-fbd7-4f38-b528-8695bd00927e",
   "metadata": {},
   "source": [
    "## LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b842b3-251b-4138-a984-6d48ffcad3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)  # pay attention to the []\n",
    "\n",
    "print(le.classes_)\n",
    "\n",
    "y_test_le = le.transform(list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dbdb42-404b-4c49-bfb4-f8ce7674ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(y_test_lb == y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5554da-5066-4996-b5a2-4ff90d18f857",
   "metadata": {},
   "source": [
    "# Tokening tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe2813-dc41-4794-a872-7dc3d273d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53255d-4ee8-4af9-a496-cace17312416",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b929a5ff-44bc-4f6a-87bc-654f50a01b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33c3ad2-1e15-4f3d-8d09-7841714db0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max([len(x) for x in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54adc751-f793-4d32-b765-2717dee57f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "\n",
    "# Last layer reflects the problem we are solving\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Embedding(\n",
    "            vocab_size, output_dim=16, mask_zero=True, input_length=maxlen\n",
    "        ),\n",
    "        tf.keras.layers.GRU(units=32, dropout=0.2, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.Dense(4, activation=\"softmax\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759fbc2-a91b-415c-aa59-e18426725df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0c6a12-8444-4be2-9235-faf2cce6a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_train, padding=\"post\", maxlen=maxlen\n",
    ")\n",
    "X_test_pad = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    X_test, padding=\"post\", maxlen=maxlen\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3cb71-2899-48d8-a23d-7cd8107ce9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664eb53f-c0e0-49b5-8cab-dc5450a6a9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad.shape, y_train_ohe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeac4bf8-c1dd-4d3c-bbda-67d1cf221089",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=10, verbose=1)\n",
    "history = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train_ohe,\n",
    "    batch_size=8,\n",
    "    epochs=1_000,\n",
    "    validation_split=0.2,\n",
    "    shuffle=True,\n",
    "    callbacks=[es],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138cee9a-b26e-4299-ba72-5f028c47c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = plt.figure(figsize=(10, 7))\n",
    "f.add_subplot()\n",
    "\n",
    "# Adding Subplot\n",
    "plt.plot(\n",
    "    history.epoch, history.history[\"accuracy\"], label=\"loss\"\n",
    ")  # Loss curve for training set\n",
    "plt.plot(\n",
    "    history.epoch, history.history[\"val_accuracy\"], label=\"val_loss\"\n",
    ")  # Loss curve for validation set\n",
    "\n",
    "plt.title(\"Loss Curve\", fontsize=18)\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0301fd-6c08-43b2-b5d9-f6ebe36db7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_pad, y_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b8f59-daed-4677-a886-8853967cddda",
   "metadata": {},
   "source": [
    "# with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c82a0-a2ec-4d40-85ab-6797318d6b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression for multi-class classification using built-in one-vs-rest\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=5,\n",
    "    n_classes=3,\n",
    "    random_state=1,\n",
    ")\n",
    "# define model\n",
    "model = LogisticRegression(multi_class=\"ovr\")\n",
    "# fit model\n",
    "model.fit(X, y)\n",
    "# make predictions\n",
    "yhat = model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f8c55-8109-4fe3-96d0-f23651478068",
   "metadata": {},
   "source": [
    "| Problem                           | Loss function        | Metrics             |\n",
    "| ---                               | ---                  |---                  |\n",
    "| Two exclusive classes             | binary_cross_entropy |                     |\n",
    "| More than 2 exclusive classes     |                      |                     |\n",
    "| More than 2 non exclusive classes |                      |                     |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
